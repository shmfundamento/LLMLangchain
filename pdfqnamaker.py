# -*- coding: utf-8 -*-
"""PDFQnAMaker

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Usp4wCULnMQrdQOfH9b_zVhIEY9KRrrS
"""

!pip install langchain
!pip install openai
!pip install PyPDF2
!pip install faiss-cpu
!pip install tiktoken

from PyPDF2 import PdfReader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import FAISS

import os
import openai
import getpass

os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')

pdfreader = PdfReader('/content/Byte - downloads.pdf')

from typing_extensions import Concatenate
# read text from pdf
raw_text = ''
for i, page in enumerate(pdfreader.pages):
    content = page.extract_text()
    if content:
        raw_text += content

# We need to split the text using Character Text Split such that it sshould not increse token size
text_splitter = CharacterTextSplitter(
    separator = "\n",
    chunk_size = 800,
    chunk_overlap  = 200,
    length_function = len,
)
texts = text_splitter.split_text(raw_text)

# Download embeddings from OpenAI
embeddings = OpenAIEmbeddings()

document_search = FAISS.from_texts(texts, embeddings)

from langchain.chains.question_answering import load_qa_chain
from langchain.llms import OpenAI

chain = load_qa_chain(OpenAI(model_name="gpt-3.5-turbo-instruct",), chain_type="stuff")

query = "Create all possible questions from the given document"#Don't say ten questions(maybe no questions or just one question)
docs = document_search.similarity_search(query)#Look into max tokens(update it(gpt works such that the total sum of input and output is 4k))
question = chain.run(input_documents=docs, question=query)#batching(context might get missed)

# Split the string into a list using '\n' as the separator
questions_list = question.split('\n')

# Remove empty strings and create a list of questions
questions = [question.strip() for question in questions_list if question.strip()]

# Print the result
print(questions)

answers = []
for q in questions:
  query = q
  docs = document_search.similarity_search(query)
  answer = chain.run(input_documents=docs, question=query)
  answers.append(answer)

print(answers)

